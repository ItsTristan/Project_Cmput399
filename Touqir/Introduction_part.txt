With the rise of data in todays digital era comes the high demand of data analysis with greater accuracies in prediction tasks. Unfortunately after a certain point, increasing the data samples doesn't necessarily contribute to higher prediction accuracy using classical machine learning techniques. Secondly, feature extraction needs to be performed for those methods which means domain knowledge must be used to get good accuracy. Neural Networks dont suffer from the second problem but for long taking full advantange of its general function approximation property has been hard leading to poorer accuracies than the state of the art methods of those times. With the advent of the first successfull deep learning technique on 2006[5], training neural networks of more than 2 hidden layers became much easier which also boosted its function approximation capacity and since then they have become the state-of-the-art algorithm for many Machine Learning tasks including Visual Recognition. To elaborate on this a bit, the main problem with training neural networks with more than 2 hidden layers was that the error function landscape would get plagued with local minimas which usually didnt give good accuracies. With shallow networks, the space of functions that can be approximated is rather quite limited and this motivates the use of deep networks. Modern deep learning techniques dont necessarily decrease the number of local minimas. However, using them, the error function usually converges to the local minimas with much lower error. A solid explanation with good mathematical reasoning hasn't been established yet but experimentally it has been seen that the architectures discover hierarchical compositional relationships inside the data which resembles how humans learn specially how the human visual recognition works. More concretely, for example in the case of image datasets, a deep convolutional neural network first learns features at the lowest levels which are the edges in its first convolutional layer and then learns a grouping of these edges which is still comparatively at a small scale in the second convolutional layer and then moves onto larger feature groupings, continuing to learn to recognize a whole object. This intuitively seems effective as images naturally have this kind of hierarchical composition which is exploited by the network. Basically the main key mechanism of these deep learning techniques is that training of each layer is first done separately. In the case of unsupervised training, each hidden layer is trained in an unsupervised fashion such as by using autoencoders and then the whole architecture is further trained as a single unit. The unsupervised learning part is often called the pretraining stage and all the other layers' weights are kept fixed during this phase. In the case of greedy learning, during the pretraining stage, the weights of a particular layer is trained in a supervised fashion with the output layer's target set to the data labels. After a layer is trained, another layer is added between it and the output layer and the process continues. Again, the weights are updated one layer at a time. Then the architecture is trained as a single unit just like before. This stage is called fine tuning. For an introduction to deep learning techniques, read the report by Touqir Sajed[6]. For a more in-depth introduction - [7]. Convolutional Nerual network was first designed by Yann Lecun back in the late 90s and it was called Lenet. Lenet was the state-of-the-art algorithm at handwritten digit recognition of its time and has been used in US postal code service for handwritten zip code recognition. For a thorough discussion, read [3]. For image classification benchmarks, the MNIST digit database has been used since the late 90s as the testing data [4]. The database contains 60,000 training images and 10,000 testing images. So far, the state-of-the-art result for MNIST digit classification has been achieved by Li Wan et al [8] which is 0.21 error rate using deep CNN and DropConnect as the regularization method.